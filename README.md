hit below endpoint in browser -
https://research-ai-agent.streamlit.app/


                                                                  AI Research Agent
AI Research Agent is a Streamlit-powered web application that leverages advanced AI and web search tools to help users research any topic efficiently. By combining LangChain, DeepSeek, and Tavily Search, the app fetches, summarizes, and synthesizes web content into concise, insightful answers directly in a conversational interface.

🚀 Features
Conversational Research: Ask any research question and get a well-structured, summarized answer with sources.
Web Search Integration: Retrieves the latest information from the web using Tavily Search.
AI-Powered Summarization: Uses advanced LLMs (DeepSeek, OpenAI) to summarize and synthesize information.
Streaming Responses: See answers generated live in a chat-like interface.
Session Memory: Maintains chat history for seamless multi-turn conversations.
User Controls: Clear chat history or stop response streaming anytime.
Fun Error Handling: Enjoy lighthearted error messages when things go wrong.
🏗️ How It Works
User submits a research query.
Web search gathers relevant results.
AI summarizes each result.
Final answer is generated by synthesizing the summaries.
Sources are displayed for transparency and further reading.
🛠️ Getting Started
Prerequisites
Python 3.8+
Streamlit
LangChain
LangGraph
Tavily Search API access
DeepSeek API access
OpenAI API access (optional)
Installation
Clone the repository:

bash
git clone https://github.com/rajeshranjan66/streamlit_ai_research.git
cd streamlit_ai_research
Install dependencies:

bash
pip install -r requirements.txt
Set up API keys:

Create a .streamlit/secrets.toml file:

TOML
LANGCHAIN_API_KEY = "your_langchain_api_key"
DEEPSEEK_API_KEY = "your_deepseek_api_key"
Running the App
bash
streamlit run ai_researcher.py
Open the URL provided by Streamlit in your browser to interact with the AI Research Agent.

💬 Usage
Enter your research query in the chat box.
Clear Chat History: Use the sidebar to clear all messages.
Stop Streaming: Interrupt a long response at any time.
🖼️ Example
AI Research Agent Screenshot

User: What are the latest advancements in quantum computing?
AI: [Concise, sourced summary with links]

📁 File Structure
ai_researcher.py — Main Streamlit application file.
🧑‍💻 Code Structure and Explanation
This section provides a block-by-block walkthrough of ai_researcher.py, explaining each part for new developers.

1. Imports and Initial Setup
Purpose:
Imports essential Python libraries and initializes Streamlit’s session state.

Highlights:

Handles regular expressions, Streamlit UI, time tracking, and UUIDs.
Imports langchain, langgraph, and Tavily/DeepSeek for LLM and search.
Sets up persistent session variables (messages for chat, stop_streaming for UI control).
Defines a list of humorous error messages for user engagement.
2. Prompt Templates
Purpose:
Defines reusable prompt templates for AI summarization and response generation.

summary_template: Summarizes search results into concise, relevant paragraphs.
generate_response_template: Guides the AI to synthesize and structure the final answer.
3. API Key and Environment Configuration
Purpose:
Sets up API keys and environment variables for LangChain and DeepSeek integration.

Loads secrets from Streamlit's secure storage.
Configures LangChain tracing and project settings for tracking and debugging.
4. ResearchState TypedDict
Purpose:
Defines a custom state object to pass data between workflow steps.

Ensures type safety for query, sources, web results, summaries, and final response.
5. Core Function Definitions
Purpose:
Implements the main logic for each step in the research workflow.

search_web(state: ResearchState)
Uses Tavily to search the web for the user’s query.
Returns URLs and raw content.
summarize_results(state: ResearchState)
Uses DeepSeek (via LangChain) to summarize each web result.
Applies the summary_template to ensure consistency.
Cleans up text artifacts.
generate_response(state: ResearchState)
Synthesizes summarized results into a single answer using generate_response_template.
Streams the response for real-time UI updates.
clean_text(text: str)
Removes unwanted tags from AI output (e.g., <think> ... </think>).
6. Workflow Construction with LangGraph
Purpose:
Chains the above functions into a modular, maintainable research workflow.

Uses StateGraph to define nodes and transitions:
Entry: search_web
Then: summarize_results
Finish: generate_response
Compiles the workflow graph for execution.
7. Streamlit UI Handling
Purpose:
Creates an interactive, user-friendly web interface.

Key UI Elements:
Sidebar Controls:
Clear chat history
Stop response streaming
Chat Display:
Shows chat history (user and assistant messages)
Chat Input:
Accepts new research queries
Message Processing:
Handles new user input, triggers workflow, and streams assistant responses live.
Displays sources and processing time.
Handles errors gracefully, resetting flags and showing fun messages.
8. Error Handling
Purpose:
Improves user experience and robustness.

Custom StreamingStoppedError to cleanly interrupt LLM output if requested.
Randomized, friendly error messages for unexpected failures.
📚 Appendix: Tips for New Developers
Add new AI models:
Update the summarization or response generation sections with new LLM integrations.
Expand search capabilities:
Swap out or augment TavilySearchResults with other APIs.
Enhance prompt engineering:
Tweak the templates for improved answer quality or a different tone.
Where to Implement New Features
New workflow steps:
Add additional nodes to the LangGraph workflow section.
UI enhancements:
Modify the Streamlit UI code blocks.
Error handling:
Expand the funny_messages list or add new exception types.
📄 License
This project is licensed under the MIT License.

🙏 Acknowledgments
LangChain
Streamlit
DeepSeek
Tavily
